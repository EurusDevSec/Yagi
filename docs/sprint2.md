# üìÖ Sprint 2 Guide: The Storm Replay (D√≤ng Ch·∫£y D·ªØ Li·ªáu)
**M·ª•c ti√™u:** T√°i hi·ªán c∆°n b√£o Yagi b·∫±ng c√°ch ƒë·∫©y d·ªØ li·ªáu t·ª´ CSV v√†o Kafka v√† l∆∞u tr·ªØ xu·ªëng Data Lake (MinIO) th√¥ng qua Spark Streaming.

---

## 1. C·∫≠p nh·∫≠t H·∫° T·∫ßng (Networking Fix)
ƒê·ªÉ Producer (ch·∫°y ·ªü Local) v√† Spark (ch·∫°y trong Docker) ƒë·ªÅu k·∫øt n·ªëi ƒë∆∞·ª£c v·ªõi Kafka, ta c·∫ßn m·ªü th√™m c·ªïng k·∫øt n·ªëi.

### B∆∞·ªõc 1.1: S·ª≠a file `docker-compose.yaml`
C·∫≠p nh·∫≠t service `kafka` nh∆∞ sau:

```yaml
  kafka:
    image: apache/kafka:latest
    container_name: yagi_kafka
    ports:
      - "9092:9092" # Internal (Spark -> Kafka) - Th·ª±c ra port n√†y d√πng trong Docker network
      - "9094:9094" # External (Local Producer -> Kafka) - M·ªöI
    environment:
      - KAFKA_NODE_ID=0
      - KAFKA_PROCESS_ROLES=controller,broker
      - KAFKA_CONTROLLER_QUORUM_VOTERS=0@yagi_kafka:9093
      - KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://yagi_kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
    volumes:
      - kafka_data:/var/lib/kafka/data
    restart: on-failure
```

*Gi·∫£i th√≠ch:*
*   `PLAINTEXT (9092)`: D√†nh cho c√°c container b√™n trong (Spark) g·ªçi t·ªõi `yagi_kafka:9092`.
*   `EXTERNAL (9094)`: D√†nh cho m√°y t√≠nh c·ªßa b·∫°n (Producer) g·ªçi t·ªõi `localhost:9094`.

### B∆∞·ªõc 1.2: Apply thay ƒë·ªïi
```bash
docker-compose up -d
```

---

## 2. Chu·∫©n B·ªã M√¥i Tr∆∞·ªùng Python (Local)
C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ ch·∫°y Producer tr√™n m√°y c·ªßa b·∫°n:

```bash
pip install pandas kafka-python
```

---

## 3. Implement Producer (Python)
T·∫°o file `jobs/yagi_producer.py`. Script n√†y ƒë·ªçc file CSV v√† b·∫Øn v√†o Kafka t·ª´ng d√≤ng m·ªôt.

**L∆∞u √Ω:** ƒê·ªïi t√™n file d·ªØ li·ªáu trong th∆∞ m·ª•c `data/` th√†nh `yagi_storm.csv` cho d·ªÖ g·ªçi, ho·∫∑c s·ª≠a ƒë∆∞·ªùng d·∫´n trong code.

```python
import time
import json
import pandas as pd
from kafka import KafkaProducer

# C·∫•u h√¨nh
KAFKA_TOPIC = "weather-stream"
KAFKA_BOOTSTRAP_SERVERS = "localhost:9094" # Port External
DATA_PATH = "../data/yagi_storm.csv" # ƒê∆∞·ªùng d·∫´n t·ªõi file CSV
SPEED_FACTOR = 1 # 1 = Real-time, 10 = Nhanh g·∫•p 10 l·∫ßn

def json_serializer(data):
    return json.dumps(data).encode("utf-8")

def run_producer():
    # 1. Kh·ªüi t·∫°o Producer
    producer = KafkaProducer(
        bootstrap_servers=[KAFKA_BOOTSTRAP_SERVERS],
        value_serializer=json_serializer
    )
    
    # 2. ƒê·ªçc d·ªØ li·ªáu
    print(f"Reading data from {DATA_PATH}...")
    df = pd.read_csv(DATA_PATH)
    # L·ªçc c√°c c·ªôt c·∫ßn thi·∫øt n·∫øu c·∫ßn (datetime, wind_kph, pressure_mb, ...)
    # df = df[['datetime', 'wind_kph', 'pressure_mb', 'precip_mm', 'humidity']]
    
    print(f"Start sending {len(df)} records to Kafka topic '{KAFKA_TOPIC}'...")
    
    for index, row in df.iterrows():
        record = row.to_dict()
        
        # G·ª≠i tin nh·∫Øn
        producer.send(KAFKA_TOPIC, record)
        print(f"Sent: {record['datetime']} - Wind: {record.get('wind_kph', 0)} km/h")
        
        # Gi·∫£ l·∫≠p delay (n·∫øu c·∫ßn ch√≠nh x√°c theo timestamp th√¨ code ph·ª©c t·∫°p h∆°n, ·ªü ƒë√¢y ta sleep t∆∞·ª£ng tr∆∞ng)
        time.sleep(1 / SPEED_FACTOR)
        
    producer.flush()
    print("Done!")

if __name__ == "__main__":
    run_producer()
```

---

## 4. Implement Spark Job (Ingestion)
T·∫°o file `jobs/spark_ingestion.py`. ƒê√¢y l√† tr√°i tim c·ªßa Pipeline, ch·∫°y b√™n trong container Spark.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType

# C·∫•u h√¨nh MinIO & Kafka
MINIO_ACCESS_KEY = "admin"
MINIO_SECRET_KEY = "password123"
MINIO_ENDPOINT = "http://yagi_minio:9000"
KAFKA_BOOTSTRAP_SERVERS = "yagi_kafka:9092" # Port Internal
TOPIC = "weather-stream"

def main():
    # 1. Kh·ªüi t·∫°o Spark Session v·ªõi Delta & S3 Support
    spark = SparkSession.builder \
        .appName("YagiStormIngestion") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # 2. ƒê·ªãnh nghƒ©a Schema (ƒêi·ªÅu ch·ªânh theo c·ªôt trong CSV c·ªßa b·∫°n)
    schema = StructType([
        StructField("datetime", StringType(), True),
        StructField("temp_c", DoubleType(), True),
        StructField("wind_kph", DoubleType(), True),
        StructField("pressure_mb", DoubleType(), True),
        StructField("precip_mm", DoubleType(), True),
        StructField("humidity", DoubleType(), True),
        StructField("cloud", DoubleType(), True),
        StructField("condition_text", StringType(), True)
    ])

    # 3. ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", TOPIC) \
        .option("startingOffsets", "earliest") \
        .load()

    # 4. Parse JSON
    parsed_df = kafka_df.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")

    # 5. Ghi d·ªØ li·ªáu xu·ªëng MinIO (Delta Lake)
    query = parsed_df.writeStream \
        .format("delta") \
        .outputMode("append") \
        .option("checkpointLocation", "s3a://yagi-data/checkpoints/weather") \
        .option("path", "s3a://yagi-data/bronze/weather") \
        .start()

    print("Spark Streaming is running... Data is flowing to MinIO.")
    query.awaitTermination()

if __name__ == "__main__":
    main()
```

---

## 5. Th·ª±c Thi Pipeline

### B∆∞·ªõc 5.1: Submit Job Spark (Trong Container)
M·ªü m·ªôt terminal m·ªõi (Git Bash ho·∫∑c CMD), ch·∫°y l·ªánh sau ƒë·ªÉ ƒë∆∞a Job v√†o Spark Master:

```bash
docker exec -it yagi_spark_master //opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf spark.jars.ivy=//tmp/.ivy \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  //opt/spark/jobs/spark_ingestion.py
```
*(L∆∞u √Ω: Tr√™n Windows Git Bash, ta c·∫ßn d√πng d·∫•u `//` ·ªü ƒë·∫ßu ƒë∆∞·ªùng d·∫´n ƒë·ªÉ tr√°nh l·ªói t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi ƒë∆∞·ªùng d·∫´n).*

### B∆∞·ªõc 5.2: Ch·∫°y Producer (Tr√™n Local)
M·ªü m·ªôt terminal kh√°c (t·∫°i folder d·ª± √°n), ch·∫°y file Python ƒë·ªÉ b·∫Øt ƒë·∫ßu b·∫Øn d·ªØ li·ªáu:

```bash
cd jobs
python yagi_producer.py
```

### B∆∞·ªõc 5.3: Ki·ªÉm Tra K·∫øt Qu·∫£
1.  Nh√¨n v√†o log c·ªßa Producer, b·∫°n s·∫Ω th·∫•y n√≥ ƒëang g·ª≠i t·ª´ng d√≤ng `Sent: ...`.
2.  M·ªü tr√¨nh duy·ªát v√†o MinIO `localhost:9001` (login `admin` / `password123`).
3.  V√†o bucket `yagi-data`, ki·ªÉm tra folder `bronze/weather`. N·∫øu th·∫•y c√°c file `.parquet` xu·∫•t hi·ªán l√† th√†nh c√¥ng! üéâ
