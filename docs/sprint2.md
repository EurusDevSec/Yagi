# üìÖ Sprint 2 Guide: The Storm Replay (D√≤ng Ch·∫£y D·ªØ Li·ªáu)
**M·ª•c ti√™u:** T√°i hi·ªán c∆°n b√£o Yagi b·∫±ng c√°ch ƒë·∫©y d·ªØ li·ªáu t·ª´ CSV v√†o Kafka v√† l∆∞u tr·ªØ xu·ªëng Data Lake (MinIO) th√¥ng qua Spark Streaming.

---

## ‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng

> **AWS SDK kh√¥ng ch·∫•p nh·∫≠n hostname c√≥ d·∫•u g·∫°ch d∆∞·ªõi (`_`).**
> 
> T·∫•t c·∫£ t√™n container **PH·∫¢I** d√πng d·∫•u g·∫°ch ngang (`-`) thay v√¨ g·∫°ch d∆∞·ªõi.
> - ‚úÖ `yagi-kafka`, `yagi-minio`, `yagi-spark-master`
> - ‚ùå `yagi_kafka`, `yagi_minio`, `yagi_spark_master`

---

## 1. C·∫•u H√¨nh Docker Compose

### File `docker-compose.yaml` (Ph·∫ßn Kafka)
```yaml
  kafka:
    image: apache/kafka:latest
    container_name: yagi-kafka  # D√πng d·∫•u g·∫°ch ngang!
    ports:
      - "9092:9092" # Internal (Spark -> Kafka)
      - "9094:9094" # External (Local Producer -> Kafka)
    environment:
      - KAFKA_NODE_ID=0
      - KAFKA_PROCESS_ROLES=controller,broker
      - KAFKA_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://yagi-kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
    volumes:
      - kafka_data:/var/lib/kafka/data
    restart: on-failure
```

*Gi·∫£i th√≠ch:*
*   `PLAINTEXT (9092)`: D√†nh cho c√°c container b√™n trong (Spark) g·ªçi t·ªõi `yagi-kafka:9092`.
*   `EXTERNAL (9094)`: D√†nh cho m√°y t√≠nh c·ªßa b·∫°n (Producer) g·ªçi t·ªõi `localhost:9094`.

### Apply thay ƒë·ªïi
```bash
docker-compose down
docker-compose up -d
```

---

## 2. Chu·∫©n B·ªã M√¥i Tr∆∞·ªùng Python (Local)
C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ ch·∫°y Producer tr√™n m√°y c·ªßa b·∫°n:

```bash
pip install pandas kafka-python
```

---

## 3. T·∫°o Bucket MinIO

Tr∆∞·ªõc khi ch·∫°y Spark job, ph·∫£i t·∫°o bucket `yagi-data` trong MinIO:

1. M·ªü tr√¨nh duy·ªát v√†o `http://localhost:9001`
2. ƒêƒÉng nh·∫≠p: `admin` / `password123`
3. T·∫°o bucket m·ªõi c√≥ t√™n `yagi-data`

Ho·∫∑c d√πng CLI:
```bash
docker exec yagi-minio mc alias set myminio http://localhost:9000 admin password123
docker exec yagi-minio mc mb myminio/yagi-data
```

---

## 4. File Producer: `jobs/yagi_producer.py`

```python
import os
import time
import json
import pandas as pd
from kafka import KafkaProducer

# C·∫•u h√¨nh
KAFKA_TOPIC = "weather-stream"
KAFKA_BOOTSTRAP_SERVERS = "localhost:9094"  # Port External 
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(SCRIPT_DIR, "../data/yagi_storm.csv")

def json_serializer(data):
    return json.dumps(data).encode("utf-8")

def run_producer():
    producer = KafkaProducer(
        bootstrap_servers=[KAFKA_BOOTSTRAP_SERVERS],
        value_serializer=json_serializer
    )
    
    print(f"Reading data from {DATA_PATH}...")
    df = pd.read_csv(DATA_PATH)
    
    print(f"Start sending {len(df)} records to Kafka topic '{KAFKA_TOPIC}'...")
    
    for index, row in df.iterrows():
        record = row.to_dict()
        producer.send(KAFKA_TOPIC, record)
        print(f"Sent: {record['datetime']} - Wind: {record.get('windspeed', 0)} km/h")
        
    producer.flush()
    print("Done!")

if __name__ == "__main__":
    run_producer()
```

---

## 5. File Spark Ingestion: `jobs/spark_ingestion.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType


MINIO_ACCESS_KEY = "admin"
MINIO_SECRET_KEY = "password123"
MINIO_ENDPOINT = "http://yagi-minio:9000"  # D√πng d·∫•u g·∫°ch ngang!
KAFKA_BOOTSTRAP_SERVERS = "yagi-kafka:9092"  # D√πng d·∫•u g·∫°ch ngang!
TOPIC = "weather-stream"


def main():
    # 1. Kh·ªüi t·∫°o Spark Session v·ªõi c·∫•u h√¨nh S3A cho MinIO
    spark = SparkSession.builder \
        .appName("YagiStormIngestion") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # 2. ƒê·ªãnh nghƒ©a Schema (kh·ªõp v·ªõi c·ªôt trong CSV)
    schema = StructType([
        StructField("name", StringType(), True),
        StructField("datetime", StringType(), True),
        StructField("temp", DoubleType(), True),
        StructField("feelslike", DoubleType(), True),
        StructField("dew", DoubleType(), True),
        StructField("humidity", DoubleType(), True),
        StructField("precip", DoubleType(), True),
        StructField("precipprob", DoubleType(), True),
        StructField("preciptype", StringType(), True),
        StructField("snow", DoubleType(), True),
        StructField("snowdepth", DoubleType(), True),
        StructField("windgust", DoubleType(), True),
        StructField("windspeed", DoubleType(), True),
        StructField("winddir", DoubleType(), True),
        StructField("sealevelpressure", DoubleType(), True),
        StructField("cloudcover", DoubleType(), True),
        StructField("visibility", DoubleType(), True),
        StructField("solarradiation", DoubleType(), True),
        StructField("solarenergy", DoubleType(), True),
        StructField("uvindex", DoubleType(), True),
        StructField("severerisk", DoubleType(), True),
        StructField("conditions", StringType(), True),
        StructField("icon", StringType(), True),
        StructField("stations", StringType(), True)
    ])

    # 3. ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", TOPIC) \
        .option("startingOffsets", "earliest") \
        .load()

    # 4. Parse JSON
    parsed_df = kafka_df.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")

    # 5. Ghi d·ªØ li·ªáu xu·ªëng MinIO (Delta Lake)
    query = parsed_df.writeStream \
        .format("delta") \
        .outputMode("append") \
        .option("checkpointLocation", "s3a://yagi-data/checkpoints/weather") \
        .option("path", "s3a://yagi-data/bronze/weather") \
        .start()

    print("Spark Streaming is running... Data is flowing to MinIO.")
    query.awaitTermination()

if __name__ == "__main__":
    main()
```

**C·∫•u h√¨nh quan tr·ªçng:**
- `spark.hadoop.fs.s3a.endpoint.region=us-east-1` - **B·∫ÆT BU·ªòC!** AWS SDK c·∫ßn config n√†y d√π MinIO kh√¥ng quan t√¢m region.

---

## 6. Th·ª±c Thi Pipeline

### üîπ B∆∞·ªõc 1: Ch·∫°y Producer TR∆Ø·ªöC (t·∫°o Kafka topic)

```bash
python jobs/yagi_producer.py
```

ƒê·ª£i cho ƒë·∫øn khi th·∫•y `Done!`.

### üîπ B∆∞·ªõc 2: Submit Spark Job

**C√°ch 1: Ch·∫°y t·ª´ TRONG container (khuy√™n d√πng)**

```bash
# V√†o container
docker exec -it yagi-spark-master bash

# Ch·∫°y spark-submit
/opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf spark.jars.ivy=/tmp/.ivy \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/jobs/spark_ingestion.py
```

**C√°ch 2: Ch·∫°y t·ª´ Git Bash (Windows)**

```bash
docker exec -it yagi-spark-master //opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf spark.jars.ivy=//tmp/.ivy \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  //opt/spark/jobs/spark_ingestion.py
```

*(L∆∞u √Ω: D√πng `//` ·ªü ƒë·∫ßu path ƒë·ªÉ tr√°nh Git Bash t·ª± chuy·ªÉn ƒë·ªïi ƒë∆∞·ªùng d·∫´n)*

### üîπ B∆∞·ªõc 3: Ki·ªÉm Tra K·∫øt Qu·∫£

1. Xem log Spark - ph·∫£i th·∫•y: `Spark Streaming is running... Data is flowing to MinIO.`
2. M·ªü MinIO Console: `http://localhost:9001` (login: `admin` / `password123`)
3. V√†o bucket `yagi-data` ‚Üí th∆∞ m·ª•c `bronze/weather` ‚Üí th·∫•y c√°c file `.parquet` l√† **TH√ÄNH C√îNG!** üéâ

---

## 7. Troubleshooting

| L·ªói | Nguy√™n nh√¢n | C√°ch fix |
|-----|-------------|----------|
| `hostname cannot be null` | Hostname c√≥ d·∫•u g·∫°ch d∆∞·ªõi `_` | ƒê·ªïi t·∫•t c·∫£ `yagi_*` th√†nh `yagi-*` trong docker-compose.yaml |
| `UnknownTopicOrPartitionException` | Kafka topic ch∆∞a t·ªìn t·∫°i | Ch·∫°y Producer tr∆∞·ªõc ƒë·ªÉ t·∫°o topic |
| `NumberFormatException: "60s"` | Spark version kh√¥ng t∆∞∆°ng th√≠ch | D√πng `apache/spark:3.5.3` |
| `ClassNotFoundException: scala.collection...` | Sai Scala version trong packages | D√πng `_2.12` cho Spark 3.5.x |

---

## 8. Phi√™n B·∫£n ƒê√£ Test Th√†nh C√¥ng

| Component | Version |
|-----------|---------|
| Spark | `apache/spark:3.5.3` |
| Kafka | `apache/kafka:latest` |
| MinIO | `minio/minio:latest` |
| Delta Lake | `delta-spark_2.12:3.1.0` |
| Hadoop AWS | `hadoop-aws:3.3.4` |
| Scala | `2.12` |
