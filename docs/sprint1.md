# ğŸ“… Sprint 1 Guide: The Foundation
**Chá»§ Ä‘á»:** XÃ¢y Dá»±ng Háº¡ Táº§ng Container (Infrastructure Layer)
**Dá»± Ã¡n:** Y.A.G.I (Yielding Adaptive Geo-spatial Intelligence)
**Tráº¡ng thÃ¡i:** ğŸš€ Ready to Start

---

## 1. Má»¥c TiÃªu (Objectives)
Má»¥c tiÃªu cá»§a Sprint nÃ y lÃ  xÃ¢y dá»±ng háº¡ táº§ng Big Data tá»‘i Æ°u cho viá»‡c tÃ¡i hiá»‡n siÃªu bÃ£o Yagi.

*   âœ… **Services:** Spark (Master/Worker), Kafka (KRaft mode), MinIO, Portainer.
*   âœ… **Constraint:** Tá»•ng lÆ°á»£ng RAM tiÃªu thá»¥ < 8GB.
*   âœ… **Outcome:** Lá»‡nh `docker-compose up -d` kÃ­ch hoáº¡t thÃ nh cÃ´ng táº¥t cáº£ services.

---

## 2. Chuáº©n Bá»‹ (Prerequisites)

### 2.1. Cáº¥u TrÃºc ThÆ° Má»¥c
HÃ£y tá»• chá»©c láº¡i folder dá»± Ã¡n cá»§a báº¡n nhÆ° sau:

```bash
Yagi/
â”œâ”€â”€ data/               # Chá»©a dá»¯ liá»‡u thÃ´ (file csv Yagi)
â”œâ”€â”€ docker-compose.yaml # File Ä‘á»‹nh nghÄ©a toÃ n bá»™ háº¡ táº§ng
â”œâ”€â”€ jobs/               # Chá»©a code Spark Job (Ingestion, Processing)
â”œâ”€â”€ notebooks/          # Chá»©a Jupyter Notebooks (Analysis/EDA)
â”œâ”€â”€ schemas/            # Chá»©a Ä‘á»‹nh nghÄ©a Schema
â””â”€â”€ services/           # Chá»©a code cÃ¡c microservices (Streamlit, API)
```

### 2.2. Copy Dá»¯ Liá»‡u
HÃ£y copy file `Hai phong, Viet Nam 2024-09-05 to 2024-09-09.csv` vÃ o thÆ° má»¥c `data/` trong project.

---

## 3. CÃ¡c BÆ°á»›c Thá»±c Hiá»‡n (Implementation Steps)

### BÆ°á»›c 1: Táº¡o file `docker-compose.yaml`
Táº¡o file `docker-compose.yaml` táº¡i thÆ° má»¥c gá»‘c. LÆ°u Ã½ bucket máº·c Ä‘á»‹nh cá»§a MinIO lÃ  `yagi-data` vÃ  Kafka cháº¡y mode KRaft.

```yaml
version: '3.8'

services:
  # --- Visualization & Monitoring ---
  portainer:
    image: portainer/portainer-ce:latest
    container_name: yagi_portainer
    ports:
      - "9000:9000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    restart: always

  # --- Message Queue (Kafka KRaft Mode - No Zookeeper) ---
  kafka:
    image: bitnami/kafka:latest
    container_name: yagi_kafka
    ports:
      - "9092:9092"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      # Listeners
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
    volumes:
      - kafka_data:/bitnami/kafka
    start_period: 30s
    restart: on-failure

  # --- Storage (MinIO - Data Lake) ---
  minio:
    image: minio/minio:latest
    container_name: yagi_minio
    ports:
      - "9000:9000" # API Port
      - "9001:9001" # Console Port
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # --- Processing (Spark) ---
  spark-master:
    image: bitnami/spark:latest
    container_name: yagi_spark_master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs

  spark-worker:
    image: bitnami/spark:latest
    container_name: yagi_spark_worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G # Limit RAM
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs

  # --- Init Job (Optional: Create Bucket Automatically) ---
  # Báº¡n cÃ³ thá»ƒ dÃ¹ng container 'mc' Ä‘á»ƒ táº¡o bucket tá»± Ä‘á»™ng,
  # hoáº·c lÃ m thá»§ cÃ´ng trong bÆ°á»›c Smoke Test.

volumes:
  kafka_data:
  minio_data:
  portainer_data:
```

### BÆ°á»›c 2: Start Services
Cháº¡y lá»‡nh:
```bash
docker-compose up -d
```

### BÆ°á»›c 3: Smoke Test & Setup Bucket
1.  **Portainer (localhost:9000):** Kiá»ƒm tra xem cáº£ 5 container (kafka, minio, spark-master, spark-worker, portainer) cÃ³ xanh khÃ´ng.
2.  **MinIO (localhost:9001):**
    *   Login: `admin` / `password123`.
    *   **QUAN TRá»ŒNG:** VÃ o menu **Buckets** -> Create Bucket -> Äáº·t tÃªn: `yagi-data` (ÄÃ¢y lÃ  nÆ¡i chá»©a dá»¯ liá»‡u bÃ£o).
3.  **Spark (localhost:8080):** Äáº£m báº£o Worker Ä‘ang Alive.

### BÆ°á»›c 4: Validation
Docker Cluster cá»§a báº¡n Ä‘Ã£ sáºµn sÃ ng tiáº¿p nháº­n dá»¯ liá»‡u bÃ¡o bÃ£o trong Sprint 2!
