# üìÖ Sprint 3 Guide: The Intelligence (MLOps Logic)
**M·ª•c ti√™u:** H·ªá th·ªëng c√≥ "n√£o" - Train model d·ª± b√°o b√£o v√† deploy prediction service ƒë·ªÉ c·∫£nh b√°o th·ªùi gian th·ª±c.

---

## üìã Y√™u C·∫ßu Tr∆∞·ªõc Khi B·∫Øt ƒê·∫ßu

- ‚úÖ Ho√†n th√†nh Sprint 2 (d·ªØ li·ªáu ƒë√£ ch·∫£y v√†o MinIO Delta Lake)
- ‚úÖ Docker containers ƒëang ch·∫°y (`docker-compose up -d`)
- ‚úÖ C√≥ Google account ƒë·ªÉ d√πng Colab

---

## 1. Train Model tr√™n Google Colab

### B∆∞·ªõc 1.1: T·∫°o Notebook m·ªõi

Truy c·∫≠p [Google Colab](https://colab.research.google.com/) v√† t·∫°o notebook m·ªõi.

### B∆∞·ªõc 1.2: Upload d·ªØ li·ªáu training

```python
from google.colab import files
uploaded = files.upload()  # Upload file yagi_storm.csv
```

### B∆∞·ªõc 1.3: Code Training Model

```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
import joblib

# 1. Load d·ªØ li·ªáu
df = pd.read_csv('yagi_storm.csv')
print(f"Dataset shape: {df.shape}")
print(df.columns.tolist())  # Xem t·∫•t c·∫£ t√™n c·ªôt
print(df.head())

# 2. Feature Engineering
# T·∫°o label: 1 = Nguy hi·ªÉm (gi√≥ > 60km/h), 0 = An to√†n
# L∆∞u √Ω: C·ªôt gi√≥ l√† 'windspeed' kh√¥ng ph·∫£i 'wind_kph'
df['is_dangerous'] = (df['windspeed'] > 60).astype(int)

# Features ƒë·ªÉ predict (d√πng ƒë√∫ng t√™n c·ªôt t·ª´ CSV)
features = ['temp', 'sealevelpressure', 'humidity', 'cloudcover', 'precip', 'windgust']
X = df[features].fillna(0)
y = df['is_dangerous']

print(f"\nDangerous records: {y.sum()} / {len(y)}")

# 3. Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Train Classification Model (Nguy hi·ªÉm hay kh√¥ng?)
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# 5. Evaluate
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy: {accuracy:.2%}")

# 6. Feature Importance
importance = pd.DataFrame({
    'feature': features,
    'importance': clf.feature_importances_
}).sort_values('importance', ascending=False)
print("\nFeature Importance:")
print(importance)

# 7. Save Model
joblib.dump(clf, 'storm_classifier.pkl')
print("\n‚úÖ Model saved to storm_classifier.pkl")

# 8. Download model
files.download('storm_classifier.pkl')
```

### B∆∞·ªõc 1.4: (Optional) Train Regression Model

N·∫øu mu·ªën d·ª± b√°o s·ª©c gi√≥ thay v√¨ ph√¢n lo·∫°i:

```python
# Regression: Predict wind speed
# D√πng ƒë√∫ng t√™n c·ªôt t·ª´ CSV
X_reg = df[['temp', 'sealevelpressure', 'humidity', 'cloudcover', 'precip', 'windgust']].fillna(0)
y_reg = df['windspeed'].fillna(0)

X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

reg = GradientBoostingRegressor(n_estimators=100, random_state=42)
reg.fit(X_train, y_train)

y_pred = reg.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE: {rmse:.2f} km/h")

joblib.dump(reg, 'storm_regressor.pkl')
files.download('storm_regressor.pkl')
```

---

## 2. Deploy Prediction Service

### B∆∞·ªõc 2.1: T·∫°o th∆∞ m·ª•c cho Predictor

```bash
mkdir -p predictor
```

### B∆∞·ªõc 2.2: Copy model v√†o th∆∞ m·ª•c

ƒê·∫∑t file `storm_classifier.pkl` (ƒë√£ download t·ª´ Colab) v√†o th∆∞ m·ª•c `predictor/`.

### B∆∞·ªõc 2.3: T·∫°o file `predictor/predictor.py`

```python
import json
import time
import joblib
import numpy as np
from kafka import KafkaConsumer, KafkaProducer

# C·∫•u h√¨nh
KAFKA_BOOTSTRAP_SERVERS = "yagi-kafka:9092"
INPUT_TOPIC = "weather-stream"
ALERT_TOPIC = "storm-alerts"
MODEL_PATH = "/app/storm_classifier.pkl"

# Ng∆∞·ª°ng c·∫£nh b√°o
WIND_THRESHOLD = 60  # km/h

def load_model():
    """Load trained model"""
    try:
        model = joblib.load(MODEL_PATH)
        print(f"‚úÖ Model loaded from {MODEL_PATH}")
        return model
    except FileNotFoundError:
        print(f"‚ö†Ô∏è Model not found at {MODEL_PATH}, using rule-based prediction")
        return None

def create_alert(record, prediction, confidence=None):
    """T·∫°o message c·∫£nh b√°o"""
    alert_level = "üî¥ CRITICAL" if prediction == 1 else "üü¢ SAFE"
    
    alert = {
        "timestamp": record.get("datetime", time.strftime("%Y-%m-%d %H:%M:%S")),
        "alert_level": alert_level,
        "is_dangerous": bool(prediction),
        "windspeed": record.get("windspeed", 0),
        "sealevelpressure": record.get("sealevelpressure", 0),
        "humidity": record.get("humidity", 0),
        "message": f"Wind speed: {record.get('windspeed', 0)} km/h"
    }
    
    if confidence is not None:
        alert["confidence"] = f"{confidence:.2%}"
    
    return alert

def rule_based_predict(record):
    """Fallback: Rule-based prediction n·∫øu kh√¥ng c√≥ model"""
    wind = record.get("windspeed", 0) or 0
    pressure = record.get("sealevelpressure", 1013) or 1013
    
    # Gi√≥ > 60km/h HO·∫∂C √°p su·∫•t < 990mb = Nguy hi·ªÉm
    if wind > WIND_THRESHOLD or pressure < 990:
        return 1
    return 0

def main():
    print("üöÄ Storm Predictor Service Starting...")
    
    # Load model
    model = load_model()
    
    # Kafka Consumer
    consumer = KafkaConsumer(
        INPUT_TOPIC,
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='latest',
        group_id='predictor-group'
    )
    
    # Kafka Producer (for alerts)
    producer = KafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
    
    print(f"üì° Listening to topic: {INPUT_TOPIC}")
    print(f"üì¢ Alerts will be sent to: {ALERT_TOPIC}")
    
    for message in consumer:
        record = message.value
        
        try:
            if model is not None:
                # ML-based prediction (d√πng ƒë√∫ng t√™n c·ªôt t·ª´ CSV)
                features = np.array([[
                    record.get('temp', 0) or 0,
                    record.get('sealevelpressure', 1013) or 1013,
                    record.get('humidity', 50) or 50,
                    record.get('cloudcover', 0) or 0,
                    record.get('precip', 0) or 0,
                    record.get('windgust', 0) or 0
                ]])
                prediction = model.predict(features)[0]
                proba = model.predict_proba(features)[0]
                confidence = max(proba)
            else:
                # Rule-based fallback
                prediction = rule_based_predict(record)
                confidence = None
            
            # T·∫°o v√† g·ª≠i alert
            alert = create_alert(record, prediction, confidence)
            producer.send(ALERT_TOPIC, alert)
            
            # Log
            status = "‚ö†Ô∏è DANGEROUS" if prediction == 1 else "‚úÖ Safe"
            print(f"{record.get('datetime')} | Wind: {record.get('windspeed', 0):>6} km/h | {status}")
            
        except Exception as e:
            print(f"‚ùå Error processing: {e}")
            continue
    
    consumer.close()
    producer.close()

if __name__ == "__main__":
    main()
```

### B∆∞·ªõc 2.4: T·∫°o file `predictor/requirements.txt`

```
kafka-python
scikit-learn
joblib
numpy
pandas
```

### B∆∞·ªõc 2.5: T·∫°o file `predictor/Dockerfile`

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Copy requirements v√† install
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy model v√† code
COPY storm_classifier.pkl .
COPY predictor.py .

# Run
CMD ["python", "predictor.py"]
```

### B∆∞·ªõc 2.6: C·∫≠p nh·∫≠t `docker-compose.yaml`

Th√™m service `predictor` v√†o cu·ªëi file (tr∆∞·ªõc `volumes:`):

```yaml
  # --- Prediction Service ---
  predictor:
    build: ./predictor
    container_name: yagi-predictor
    depends_on:
      - kafka
    restart: on-failure
    environment:
      - PYTHONUNBUFFERED=1
```

---

## 3. Th·ª±c Thi

### B∆∞·ªõc 3.1: Build v√† start Predictor

```bash
# Build image
docker-compose build predictor

# Start all services
docker-compose up -d
```

### B∆∞·ªõc 3.2: Ki·ªÉm tra Predictor ƒëang ch·∫°y

```bash
docker logs -f yagi-predictor
```

B·∫°n s·∫Ω th·∫•y:
```
üöÄ Storm Predictor Service Starting...
‚úÖ Model loaded from /app/storm_classifier.pkl
üì° Listening to topic: weather-stream
üì¢ Alerts will be sent to: storm-alerts
```

### B∆∞·ªõc 3.3: Ch·∫°y Producer ƒë·ªÉ test

```bash
python jobs/yagi_producer.py
```

### B∆∞·ªõc 3.4: Xem k·∫øt qu·∫£ prediction

```bash
docker logs -f yagi-predictor
```

Output s·∫Ω nh∆∞:
```
2024-09-05T00:00:00 | Wind:      0 km/h | ‚úÖ Safe
2024-09-05T01:00:00 | Wind:      0 km/h | ‚úÖ Safe
...
2024-09-07T15:00:00 | Wind:     85 km/h | ‚ö†Ô∏è DANGEROUS
2024-09-07T16:00:00 | Wind:    102 km/h | ‚ö†Ô∏è DANGEROUS
```

---

## 4. ƒê·ªçc Alert Topic (Optional)

T·∫°o file `jobs/alert_consumer.py` ƒë·ªÉ xem c√°c c·∫£nh b√°o:

```python
import json
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'storm-alerts',
    bootstrap_servers='localhost:9094',
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    auto_offset_reset='earliest'
)

print("üîî Listening for alerts...")
for msg in consumer:
    alert = msg.value
    if alert['is_dangerous']:
        print(f"üî¥ {alert['timestamp']} | {alert['message']} | {alert.get('confidence', 'N/A')}")
    else:
        print(f"üü¢ {alert['timestamp']} | {alert['message']}")
```

Ch·∫°y:
```bash
python jobs/alert_consumer.py
```

---

## 5. C·∫•u Tr√∫c Th∆∞ M·ª•c Sau Sprint 3

```
Yagi/
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ yagi_storm.csv
‚îú‚îÄ‚îÄ jobs/
‚îÇ   ‚îú‚îÄ‚îÄ yagi_producer.py
‚îÇ   ‚îú‚îÄ‚îÄ spark_ingestion.py
‚îÇ   ‚îî‚îÄ‚îÄ alert_consumer.py      # M·ªõi
‚îú‚îÄ‚îÄ predictor/                  # M·ªõi
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ predictor.py
‚îÇ   ‚îî‚îÄ‚îÄ storm_classifier.pkl   # Model t·ª´ Colab
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ plan.md
    ‚îú‚îÄ‚îÄ sprint1.md
    ‚îú‚îÄ‚îÄ sprint2.md
    ‚îî‚îÄ‚îÄ sprint3.md
```

---

## 6. Troubleshooting

| L·ªói | Nguy√™n nh√¢n | C√°ch fix |
|-----|-------------|----------|
| `Model not found` | Ch∆∞a copy file `.pkl` v√†o predictor/ | Copy model t·ª´ Colab v√†o th∆∞ m·ª•c predictor/ |
| `NoBrokersAvailable` | Kafka ch∆∞a s·∫µn s√†ng | ƒê·ª£i Kafka kh·ªüi ƒë·ªông xong ho·∫∑c th√™m retry logic |
| `docker-compose build` l·ªói | Thi·∫øu Dockerfile ho·∫∑c requirements.txt | Ki·ªÉm tra l·∫°i c·∫•u tr√∫c th∆∞ m·ª•c predictor/ |

---

## 7. K·∫øt Qu·∫£ Mong ƒê·ª£i

- [x] Model ƒë∆∞·ª£c train tr√™n Colab v·ªõi accuracy > 80%
- [x] Predictor service ch·∫°y trong Docker
- [x] Predictor consume t·ª´ `weather-stream`, predict, v√† publish l√™n `storm-alerts`
- [x] Console hi·ªÉn th·ªã tr·∫°ng th√°i Safe/Dangerous cho m·ªói b·∫£n ghi

---

## ‚û°Ô∏è Ti·∫øp theo: Sprint 4

Sprint 4 s·∫Ω t·∫≠p trung v√†o:
- Dashboard Streamlit hi·ªÉn th·ªã real-time
- T√≠ch h·ª£p Telegram Bot g·ª≠i c·∫£nh b√°o
- Chaos Engineering Test (t·∫Øt container, ki·ªÉm tra t·ª± ph·ª•c h·ªìi)
